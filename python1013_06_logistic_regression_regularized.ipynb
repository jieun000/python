{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323366ac",
   "metadata": {},
   "source": [
    "# Optional Lab - Regularized Cost and Gradient\n",
    "\n",
    "## Goals\n",
    "In this lab, you will:\n",
    "- 이전의 linear 와 logistic cost함수에 regularization term 을 추가\n",
    "- 이전에 과적합된 상태에서 regularization term을 추가되어 적용된 결과 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4ee788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from plt_overfit import overfit_example, output\n",
    "from lab_utils_common import sigmoid\n",
    "np.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850047eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최소값(argmin), 최대값(argmax), 혹은 조건(where)에 해당하는 색인(index) 값을 찾기\n",
    "np.argmin([11,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0368acce",
   "metadata": {},
   "source": [
    "# Adding regularization\n",
    "<img align=\"Left\" src=\"./images/C1_W3_LinearGradientRegularized.png\"  style=\" width:400px; padding: 10px; \" >\n",
    "<img align=\"Center\" src=\"./images/C1_W3_LogisticGradientRegularized.png\"  style=\" width:400px; padding: 10px; \" >\n",
    "\n",
    "linear 과 logistic regression의 cost와 gradient 함수:\n",
    "- Cost\n",
    "    - 선형 회귀와 로지스틱 회귀의 코스트 함수가 서로 상당히 다르지만 regularization 성분은 동일하다. \n",
    "- Gradient\n",
    "    - 선형 회귀와 로지스틱 회귀의 기울기 함수는 매우 유사하다. \n",
    "        - $f_{wb}$ 의 구현만 다를 뿐이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d710e8",
   "metadata": {},
   "source": [
    "## regularization 을 적용한 Cost 함수\n",
    "### regularized(정규화된) linear regression(선형 회귀) Cost 함수\n",
    "\n",
    "regularized 선형 회귀의 cost 함수의 공식은 :\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{1}$$ \n",
    "여기서:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{2} $$ \n",
    "정규화 되지 않은 cost 함수와 비교해보자: \n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$ \n",
    "\n",
    "regularization 성분의 차이  <span style=\"color:blue\">\n",
    "    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span> \n",
    "    \n",
    "이 성분을 이용하여 파라미터의 크기를 최소화하도록 경사 하강에 영향을 준다. \n",
    "주의, $b$ 는 정규화되지 않았다, 실무에서의 표준사항\n",
    "\n",
    "(1) 과 (2) 의 구현 코드. *standard pattern for this course* 은 이 과정의 표준 모든 `m` 개의 training data에 대하여 `for loop` 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce147ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m  = X.shape[0]\n",
    "    n  = len(w)\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b #(n,)(n,) = 스칼라\n",
    "        cost += (f_wb_i - y[i]) **2 # 스칼라\n",
    "    cost /= (2*m) # 스칼라\n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2) # 스칼라\n",
    "    reg_cost = (lambda_ / (2*m)) * reg_cost # 스칼라\n",
    "    total_cost = cost + reg_cost\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a28e0ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized 비용: 0.07917239320214275\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,) - 0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "print(\"Regularized 비용:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57aa932",
   "metadata": {},
   "source": [
    "### regularization를 활용한 미분 계산 (linear/logistic)\n",
    "linear 과 logistic regression 의 미분의 거의 동일, $f_{\\mathbf{w}b}$의 연산에서만 차이.\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{2} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n",
    "\\end{align*}$$\n",
    "\n",
    "* m 은 training data의 개수      \n",
    "* $f_{\\mathbf{w},b}(x^{(i)})$ 은 모델의 출력 결과, $y^{(i)}$ 는 target(정답/label)\n",
    "\n",
    "      \n",
    "* <span style=\"color:blue\"> **linear** </span> 회귀 모델에 대하여\n",
    "    $f_{\\mathbf{w},b}(x) = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
    "* <span style=\"color:blue\"> **logistic** </span> 회귀 모델에 대하여\n",
    "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
    "    $f_{\\mathbf{w},b}(x) = g(z)$  \n",
    "    여기서 $g(z)$ 는 sigmoid 함수:  \n",
    "    $g(z) = \\frac{1}{1+e^{-z}}$   \n",
    "    \n",
    "regularization 을 더한 것은 <span style=\"color:blue\">$\\frac{\\lambda}{m} w_j $</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7615959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_linear_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]                 \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6be1570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.6648774569425726\n",
      "Regularized dj_dw: \n",
      " [0.20266669606324647, 0.43274529026040554, 0.13824219937625334]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\",)\n",
    "print(f\"Regularized dj_dw: \\n {dj_dw_tmp.tolist()}\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d802a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff3e8b2f",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "```\n",
    "dj_db: 0.6648774569425726\n",
    "Regularized dj_dw:\n",
    " [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99fdfd3",
   "metadata": {},
   "source": [
    "In the plot above, try out regularization on the previous example. In particular:\n",
    "- Categorical (logistic regression)\n",
    "    - set degree to 6, lambda to 0 (no regularization), fit the data\n",
    "    - now set lambda to 1 (increase regularization), fit the data, notice the difference.\n",
    "- Regression (linear regression)\n",
    "    - try the same procedure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
