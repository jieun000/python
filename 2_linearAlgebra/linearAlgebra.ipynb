{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5910a34",
   "metadata": {},
   "source": [
    "Menu\n",
    "<div>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td rowspan=\"5\">linear regression</td>\n",
    "            <td>ì„ í˜•íšŒê·€ë€?</td>\n",
    "        </tr>\n",
    "        <tr><td>ê²½ì‚¬í•˜ê°•ë²•</td></tr>\n",
    "        <tr><td>ì„ í˜•íšŒê·€</td></tr>\n",
    "        <tr><td>ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ê°€ í•˜ë‚˜ì¼ ë•Œ</td></tr>\n",
    "        <tr><td>ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ê°€ ì—¬ëŸ¬ ê°œì´ë©°  ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘–ğ‘›ğ‘”ğ‘‘ğ‘ğ‘¡ğ‘(6ìŒ)</td></tr>\n",
    "    </table>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <td rowspan=\"5\">logistic regression</td>\n",
    "            <td>ë¡œì§€ìŠ¤í‹± íšŒê·€ë€?</td>\n",
    "        </tr>\n",
    "        <tr><td>Hessian í–‰ë ¬</td></tr>\n",
    "        <tr><td>1</td></tr>\n",
    "        <tr><td>2</td></tr>\n",
    "        <tr><td>3</td></tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dbfb01",
   "metadata": {},
   "source": [
    "## ì„ í˜•íšŒê·€(linear regression)ë€?\n",
    "- featureì™€ labelì€ í•­ìƒ ìŒìœ¼ë¡œ ì£¼ì–´ì§€ê³ , featureê°€ í•˜ë‚˜ì¸ ê²ƒì„ simple linear regressionì´ë¼ê³  í•˜ê³ \n",
    "- featureê°€ ì—¬ëŸ¬ ê°œ ìˆê³  labelì´ í•˜ë‚˜ ìˆëŠ” ê²ƒì„ multivariable linear regressionì´ë¼ê³  í•¨.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>x(feature/ì‹œê°„)</th>\n",
    "        <th>y(label(ì •ë‹µ)/ì ìˆ˜)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>10</td>\n",
    "        <td>90</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>9</td>\n",
    "        <td>8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>30</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "ë°ì´í„°ê°€ ì—¬ëŸ¬ ê°œê°€ ìˆê³  ì—¬ëŸ¬ ê°œì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ì ìœ¼ë¡œ ìƒê°í•˜ê³  ë²¡í„°ë¡œ ìƒê°í•˜ì <br>\n",
    "ì—¬ëŸ¬ ê°œì˜ ì ê³¼ ê°€ì¥ ì˜¤ì°¨ê°€ ì ê²Œ ë˜ëŠ” ì§ì„ ì˜ ë°©ì •ì‹(hypothesis / $ \\hat{y} $) ê³¼ì˜ ì°¨ì´ê°€ ì ê²Œ $ \\hat{y} $ ì´ ê³„ì† ì›€ì§ì´ë©´ì„œ ì˜¤ì°¨ë¥¼ ì ê²Œ í•˜ë ¤ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ(ê°±ì‚°, wì™€ bë¥¼ ë³€ê²½)í•˜ë©´ì„œ cost(ì˜¤ì°¨, Mean Square Error)ê°€ 0ìœ¼ë¡œ ê°€ê¹Œì›Œì§€ë„ë¡ í•™ìŠµì„ í•˜ê²Œ ë˜ë©´ ìš°ë¦¬ê°€ ì°¾ê³ ì í•˜ëŠ” y(ì •ë‹µ)ê³¼ ê°€ì¥ ê°€ê¹Œìš´ $ \\hat{y} $ ì„ ì°¾ê²Œë˜ëŠ” ê²ƒì´ê³  ê·¸ê²ƒì´ linear regressionì˜ í•™ìŠµì´ ì™„ë£Œëœ ê²ƒì´ë‹¤.-\n",
    "$$ J = \\frac{1}{2m}\\sum_{i=1}^{m}(y-\\hat{y})^{2} \\rightarrow MSE $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45797a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80754ba7",
   "metadata": {},
   "source": [
    "# Optional Lab: ì„ í˜•íšŒê·€ë¥¼ ìœ„í•œ ê²½ì‚¬ í•˜ê°•ë²• <br>(Gradient Descent for Linear Regression)\n",
    "\n",
    "<figure>\n",
    "    <center> <img src=\"./images(1)/C1_W1_L4_S1_Lecture_GD.png\"  style=\"width:800px;height:200px;\" ></center>\n",
    "    <figcaption>$$ pred = x \\cdot w + b $$</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c0656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(self, x, y):\n",
    "    pred = np.dot(x, self.w)+ self.b # ì˜ˆì¸¡í•œ ì§ì„ ì˜ ë°©ì •ì‹(y^hat, hypothesis(ê°€ì„¤))\n",
    "    e = y - pred # ì˜¤ì°¨\n",
    "    return np.mean(e**2) # Mean Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e51dad",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\\begin{align*} \\text{ ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë°˜ë³µ:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self,x,y): # ë°˜ë³µí•˜ì—¬ w, bë¥¼ ì°¾ìŒ\n",
    "    pred = x@self.w + self.b\n",
    "    e = y - pred\n",
    "    dJ_dw = (np.mean(e*(-2*x), axis=0)) # Jë¥¼ wë¡œ í¸ë¯¸ë¶„, 0or1ì— ë”°ë¼ ê°€ë¡œë¡œ ì„¸ë¡œë¡œ\n",
    "    dJ_db = (np.mean(e*(-2), axis=0)) # Jë¥¼ bë¡œ í¸ë¯¸ë¶„\n",
    "    self.w = (self.w.T - self.lr*dJ_dw).T # wê°±ì‹ \n",
    "    self.b = (self.b.T - self.lr*dJ_db) # bê°±ì‹ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2b230",
   "metadata": {},
   "source": [
    "*gradient descent(ê²½ì‚¬ í•˜ê°•)*  ëŠ” ì•„ë˜ì™€ ê°™ë‹¤\n",
    "\n",
    "$$\\begin{align*} \\text{ ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë°˜ë³µ:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "ì—¬ê¸°ì„œ íŒŒë¼ë¯¸í„°ëŠ” $w$, $b$ ì´ê³  ë™ì‹œì— ê°±ì‹ ë¨. \n",
    "ê¸°ìš¸ê¸°ëŠ” ì•„ë˜ì™€ ê°™ë‹¤:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    " *simultaniously* ëŠ” ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ê°±ì‹ í•˜ê¸° ì „ì— ëª¨ë“  íŒŒë¼ë¯¸í„°ì˜ í¸ë¯¸ë¶„ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•¨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bde2ab",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J(w,b)}{\\partial w} = ? $$\n",
    "\n",
    "$$ J = \\frac{1}{2m}\\sum_{i=1}^{m}(y-\\hat{y})^{2} \\rightarrow MSE $$\n",
    "$$ J = \\frac{1}{2m}\\sum_{i=1}^{m}(y-(wx+b))^{2} \\rightarrow MSE $$\n",
    "$$ wx+b \\rightarrow p $$\n",
    "$$ y-\\hat{y} \\rightarrow q $$\n",
    "$$ y-p \\rightarrow q $$\n",
    "$$  J = \\frac{1}{2m}\\sum_{i=1}^{m}q^{2} $$\n",
    "$$ \\frac{\\partial J(w,b)}{\\partial q} =  \\frac{1}{2m}\\sum_{i=1}^{m} 2q $$\n",
    "$$ \\frac{\\partial p}{\\partial w} = x $$\n",
    "$$ \\frac{\\partial J}{\\partial q} \\frac{\\partial q}{\\partial p} \\frac{\\partial p}{\\partial w} = $$\n",
    "$$ \\frac{1}{m}\\sum_{i=1}^{m}(y-\\hat{y}) \\cdot (-1) \\cdot x  $$\n",
    "$$ -\\frac{1}{m}\\sum_{i=1}^{m}(y-\\hat{y}) \\cdot x  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80aa28f",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J(w,b)}{\\partial b} = ? $$\n",
    "\n",
    "$$ J = \\frac{1}{2m}\\sum_{i=1}^{m}(y-\\hat{y})^{2} \\rightarrow MSE $$\n",
    "$$ J = \\frac{1}{2m}\\sum_{i=1}^{m}(y-(wx+b))^{2} \\rightarrow MSE $$\n",
    "$$ wx+b \\rightarrow p $$\n",
    "$$ y-\\hat{y} \\rightarrow q $$\n",
    "$$ y-p \\rightarrow q $$\n",
    "$$ \\frac{\\partial J}{\\partial b} =  \\frac{\\partial J}{\\partial q} \\frac{\\partial q}{\\partial p} \\frac{\\partial p}{\\partial b} = $$\n",
    "$$  J = \\frac{1}{2m}\\sum_{i=1}^{m}q^{2} $$\n",
    "$$ \\frac{\\partial J(w,b)}{\\partial q} =  \\frac{1}{2m}\\sum_{i=1}^{m} 2q $$\n",
    "$$ \\frac{\\partial p}{\\partial w} = x $$\n",
    "$$ \\frac{\\partial q}{\\partial p} = -1 $$\n",
    "$$ \\frac{1}{m}\\sum_{i=1}^{m}(y-\\hat{y}) \\cdot (-1) \\cdot x  $$\n",
    "$$ -\\frac{1}{m}\\sum_{i=1}^{m}(y-\\hat{y}) \\cdot x  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c79b9",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011ca2d",
   "metadata": {},
   "source": [
    "<img src='./images(1)/linear1.gif' width=\"30%\" style=\"float: left;\"/>\n",
    "<img src='./images(1)/linear2.gif' width=\"30%\" style=\"float: left;\"/>\n",
    "<img src='./images(1)/linear3.gif' width=\"30%\" style=\"float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install celluloid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9647144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import celluloid \n",
    "from celluloid import Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021edac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, w=1, b=1, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.w = np.array([[w]]) # w: í–‰ë ¬\n",
    "        self.b = np.array([[b]]) # b: ë²¡í„°\n",
    "    def cost(self, x, y):\n",
    "        pred = x@self.w + self.b # ì˜ˆì¸¡í•œ ì§ì„ ì˜ ë°©ì •ì‹(y^hat, hypothesis(ê°€ì„¤))\n",
    "        e = y - pred # ì˜¤ì°¨\n",
    "        return np.mean(e**2) # Mean Square Error\n",
    "    def fit(self,x,y): # ë°˜ë³µí•˜ì—¬ w, bë¥¼ ì°¾ìŒ\n",
    "        pred = x@self.w + self.b\n",
    "        e = y - pred\n",
    "        dJ_dw = (np.mean(e*(-2*x), axis=0)) # Jë¥¼ wë¡œ í¸ë¯¸ë¶„, 0or1ì— ë”°ë¼ ê°€ë¡œë¡œ ì„¸ë¡œë¡œ\n",
    "        dJ_db = (np.mean(e*(-2), axis=0)) # Jë¥¼ bë¡œ í¸ë¯¸ë¶„\n",
    "        self.w = (self.w.T - self.lr*dJ_dw).T # wê°±ì‹ \n",
    "        self.b = (self.b.T - self.lr*dJ_db) # bê°±ì‹ \n",
    "    def predict(self, x):\n",
    "        return (x@self.w.T + self.b) # ì˜ˆì¸¡ê°’ ë°˜í™˜\n",
    "    def params(self):\n",
    "        return (self.w, self.b) # ìš°ë¦¬ê°€ ì°¾ê³ ì í•˜ëŠ” íŒŒë¼ë¯¸í„° ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce training data\n",
    "x_train = np.array([[1],[2],[4],[5],[6],[7]])\n",
    "y_train = np.array([[4],[-12],[3],[-11],[-5],[-17]])\n",
    "\n",
    "w_list = [] # weight(ê¸°ìš¸ê¸°) ì €ì¥\n",
    "b_list = [] # bias(yì ˆí¸ ì €ì¥)\n",
    "c_list = [] # cost ì €ì¥\n",
    "\n",
    "ys_list = [] # xsì— ëŒ€í•œ ì˜ˆì¸¡ê°’ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸\n",
    "cl_list = [] # x_trainì„ ìœ„í•œ ì˜ˆì¸¡í•´ yê°’ì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸\n",
    "xs = np.array([ # regression ì§ì„ ì„ ê·¸ë¦¬ê¸° ìœ„í•œ x_value ì„¤ì •\n",
    "    [-3],\n",
    "    [10]\n",
    "])\n",
    "# ëª¨ë¸ í•™ìŠµ(training)\n",
    "model = LinearRegression(w=3, b=1, lr=0.001)\n",
    "for i in range(60000): # ë°˜ë³µ(epochs) íšŸìˆ˜ ì§€ì •\n",
    "    w_list.append(model.params()[0]) # ê¸°ìš¸ê¸° ì €ì¥\n",
    "    b_list.append(model.params()[1]) # yì ˆí¸ ì €ì¥\n",
    "    c_list.append(model.cost(x_train, y_train)) # costë¥¼ ì €ì¥\n",
    "    ys_list.append(model.predict(xs).T) # xsì— í•´ë‹¹í•˜ëŠ” y ì˜ˆì¸¡ê°’ ì €ì¥\n",
    "    cl_list.append(model.predict(x_train).T) # x_tarinì— í•´ë‹¹í•˜ëŠ” yê°’ ì €ì¥\n",
    "    model.fit(x_train, y_train)\n",
    "print('weight: ' + str(model.params()[0]))\n",
    "print('y_ì ˆí¸: ' + str(model.params()[1]))\n",
    "print('costs: ' + str(model.cost(x_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef17fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(x_train, y_train)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde83b78",
   "metadata": {},
   "source": [
    "### ì‹œê°í™” ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b877a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(0,50,1).tolist()\n",
    "b = np.arange(50,100,5).tolist()\n",
    "c = np.arange(100,12000,200).tolist()\n",
    "# 0~50ê¹Œì§€ëŠ” 1ê°„ê²©, 50~100ê¹Œì§€ëŠ” 5ê°„ê²©, 100ì´í›„ë¡œëŠ” 200ê°„ê²©ìœ¼ë¡œ ê·¸ë¦¼ì„ ê·¸ë¦¬ì\n",
    "p = a + b + c\n",
    "# ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì—´ë¡œ ë°˜í™˜\n",
    "w = np.array(w_list).flatten() # flatten(í¼ì¹œë‹¤)\n",
    "b = np.array(b_list).flatten() # flatten(í¼ì¹œë‹¤)\n",
    "c = np.array(c_list).flatten() # flatten(í¼ì¹œë‹¤)\n",
    "ys = np.array(ys_list)\n",
    "p = np.array(p)\n",
    "# ì²«ë²ˆì§¸ animation ìƒì„±\n",
    "fig = plt.figure(figsize=(10,10)) # figure ìƒì„±\n",
    "labelsize_ = 14\n",
    "camera = Camera(fig) # ì¹´ë©”ë¼ ìƒì„±\n",
    "for i in p:\n",
    "    ax1 = fig.add_subplot(3,2,2)\n",
    "    ax1.plot(w[:i], color='blue', linestyle='dashed', alpha=0.5)\n",
    "    ax1.set_title('w', fontsize=17)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=labelsize_)\n",
    "    \n",
    "    ax2 = fig.add_subplot(3,2,4)\n",
    "    ax2.plot(w[:i], color='red', linestyle='dashed', alpha=0.5)\n",
    "    ax2.set_title('b', fontsize=17)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=labelsize_)\n",
    "    \n",
    "    ax3 = fig.add_subplot(3,2,6)\n",
    "    ax3.plot(w[:i], color='black', linestyle='dashed', alpha=0.5)\n",
    "    ax3.set_title('costs', fontsize=17)\n",
    "    ax3.tick_params(axis='both', which='major', labelsize=labelsize_)\n",
    "    ax3.set_xlabel('epochs', fontsize=14, labelpad=10)\n",
    "    ax0 = fig.add_subplot(1,2,1) # fit ê·¸ë¦¬ê¸°\n",
    "    leg = ax0.plot(xs.T.flatten(), ys[i].flatten(), color='r'\n",
    "                   , label=str(i)) # legend ì„¤ì •, ê·¸ë¦¼ì„ ê·¸ë¦¬ê¸° ìœ„í•´ ë°°ì—´ flatten(í¼ì¹˜ë‹¤)\n",
    "    ax0.scatter(x_train, y_train, color='b', marker='x', s=44)\n",
    "    ax0.legend(leg, [f'eposhs: (i)'], loc='upper right', fontsize=15)\n",
    "    ax0.set_title('Linear fit', fontsize=25)\n",
    "    ax0.tick_params(axis='both', which='major', labelsize=labelsize_)\n",
    "    ax0.set_xlabel('x', fontsize=25, labelpad=10)\n",
    "    ax0.set_ylabel('y', fontsize=25, labelpad=10)\n",
    "    ax0.tick_params(axis='both', which='major', labelsize=labelsize_)\n",
    "    ax0.set_ylim([-20,10])\n",
    "    plt.tight_layout()\n",
    "    camera.snap() # ê°ê°ì˜ í”„ë ˆì„ ë° ë°˜ë³µí›„ì— ìŠ¤ëƒ…ìƒ· ì·¨í•¨\n",
    "animation = camera.animate(interval=5, repeat=False, repeat_delay=500) # ì¹´ë©”ë¼ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±\n",
    "animation.save('image/SimpleLinReg_1.gif', writer='imagemagick') # ì• ë‹ˆë©”ì´ì…˜ ì €ì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48261c9",
   "metadata": {},
   "source": [
    "-> pycharm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e2f73",
   "metadata": {},
   "source": [
    "<img src='./images(1)/20231010_150423_132.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea007a",
   "metadata": {},
   "source": [
    "$$ J = \\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2 \\rightarrow MSE $$\n",
    "$$ \\frac{\\partial J} {\\partial w} = -\\frac{1}{2n}\\sum_{i=1}^{n}(y-\\hat{y})x $$\n",
    "$$ \\frac{\\partial J} {\\partial b} = -\\frac{1}{2n}\\sum_{i=1}^{n}(y-\\hat{y}) $$\n",
    "$$ w \\leftarrow w - \\alpha \\frac{\\partial J} {\\partial w} $$\n",
    "$$ b \\leftarrow b - \\alpha \\frac{\\partial J} {\\partial b} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808bf44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[1],[2],[4],[5],[6],[7]])\n",
    "y_train = np.array([[4],[-12],[3],[-11],[-5],[-17]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ = 3\n",
    "b_ = 1\n",
    "\n",
    "for i in range(50000):\n",
    "    y_hat = w_ * x_train + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat), y^ = Wx + b\n",
    "    n = len(x_train) # ë°ì´í„° ê¸¸ì´\n",
    "    \n",
    "    # ë¯¸ë¶„\n",
    "    D_m = (-1/n) * sum(x_train * (y_train - y_hat)) # cost(J) ë¥¼ Wë¡œ í¸ë¯¸ë¶„(ê¸°ìš¸ê¸°)\n",
    "    D_b = (-1/n) * sum(y_train - y_pred) # cost(J) ë¥¼ bë¡œ í¸ë¯¸ë¶„ (yì ˆí¸)\n",
    "\n",
    "    # íŒŒë¼ë¯¸í„° ê°±ì‹ \n",
    "    w_ = w_ - 0.01 * D_m # wê°±ì‹ \n",
    "    b_ = b_ - 0.01 * D_b # bê°±ì‹ \n",
    "    \n",
    "    c_ = (1/n) * sum( (y_train - y_hat)**2 ) # cost = MSE\n",
    "    if i%1000==0:\n",
    "        print(w_, b_, c_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f87207",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611dd802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_w(x, y, w_, b_):\n",
    "    y_hat = w_ * x + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat), y^ = Wx + b\n",
    "    n = len(x) # ë°ì´í„° ê¸¸ì´\n",
    "    result = 0\n",
    "    for x_i, y_i, y_hat_i in zip(x, y, y_hat):\n",
    "        result += (y_i- y_hat_i) * x_i\n",
    "    return 1/(2*n)*result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e736a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_b(x, y, w_, b_):\n",
    "    y_hat = w_ * x + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat), y^ = Wx + b\n",
    "    n = len(x) # ë°ì´í„° ê¸¸ì´\n",
    "    result = 0\n",
    "    for y_i, y_hat_i in zip(y, y_hat):\n",
    "        result += (y_i - y_hat_i)\n",
    "    return 1/(2*n)*result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x,y,w_,b_):\n",
    "    y_hat = w_ * x + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat), y^ = Wx + b\n",
    "    n = len(x) # ë°ì´í„° ê¸¸ì´\n",
    "    result = 0\n",
    "    for y_i, y_hat_i in zip(y, y_hat):\n",
    "        result += (y_i - y_hat_i)\n",
    "    return 1/(2*n)*result\n",
    "    c_ = (1/n) * sum( (y_train - y_hat)**2 ) # cost = MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a435935",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ = 3\n",
    "b_ = 1\n",
    "\n",
    "for i in range(50000):\n",
    "    # D_b = (-1/n) * sum(y_train - y_pred) # cost(J) ë¥¼ bë¡œ í¸ë¯¸ë¶„ (yì ˆí¸)\n",
    "    dJ_dw = grade_w(x_train, y_train, w_, b_)\n",
    "    dJ_db = grade_b(x_train, y_train, w_, b_)\n",
    "    \n",
    "    # íŒŒë¼ë¯¸í„° ê°±ì‹ \n",
    "    w_ = w_ - 0.01 * D_m # wê°±ì‹ \n",
    "    b_ = b_ - 0.01 * D_b # bê°±ì‹ \n",
    "    c_ = cost(x_train, y_train, w_, b_)\n",
    "    \n",
    "    if i%1000==0:\n",
    "        print(w_, b_, c_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98c0b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd90e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression model:\n",
    "class LinearRegression(object):\n",
    "    def __init__(self,w=1,b=1, lr=0.01):\n",
    "        self.lr=lr\n",
    "        self.w=np.array([[w]])\n",
    "        self.b=np.array([b])\n",
    "\n",
    "    def cost(self,x,y):\n",
    "        pred = x@self.w+self.b  # predicted y-values\n",
    "        e=y-pred             # error term\n",
    "        return np.mean(e**2)  # mean squared error\n",
    "\n",
    "    def fit(self, x,y):\n",
    "        pred = x@self.w+self.b\n",
    "        e=y-pred\n",
    "        dJ_dw=(np.mean(e*(-2*x), axis=0)) # partial derivate of J with respect to w\n",
    "        dJ_db=(np.mean(e*(-2),axis=0)) # partial derivate of J with respect to b\n",
    "        self.w = (self.w.T-self.lr*dJ_dw).T  # update w\n",
    "        self.b = self.b - self.lr*dJ_db    # update b\n",
    "\n",
    "    def predict(self, x):\n",
    "        return (x @ self.w.T + self.b)  # return predicted values\n",
    "\n",
    "    def params(self):\n",
    "        return (self.w,self.b)   # return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d4a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import celluloid\n",
    "from celluloid import Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57925a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce training data\n",
    "x_train = np.array([[1],[2],[4],[5],[6],[7]])\n",
    "y_train = np.array([[4],[-12],[3],[-11],[-5],[-17]])\n",
    "print('x_train:', x_train,'\\ny_train:', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb15b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce lists where data points are being stored:\n",
    "w_list=[]   # list contains weights\n",
    "b_list=[]   # list contains biases\n",
    "c_list=[]   # list contains costs\n",
    "ys_list=[]  # store arrays of predicted y-values for xs ( -> plot regression line!)\n",
    "cl_list = [] # list contains predicted y-values for x_train ( -> plot connecting lines!)\n",
    "\n",
    "# set x-values for regression line plot\n",
    "xs= np.array([[-3],[10]])\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c848df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model:\n",
    "model=LinearRegression(w=3,b=-1,lr=0.001) # set initial parameters and learning rate\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca09fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60000):      # set number of epochs\n",
    "    w_list.append(model.params()[0])    # append weights (=slopes) to list\n",
    "    b_list.append(model.params()[1])    # append biases (=y-intercepts) to list\n",
    "    c_list.append(model.cost(x_train,y_train))  # append costs to list\n",
    "    ys_list.append(model.predict(xs).T)     # append pairs of predicted y-values for xs\n",
    "    cl_list.append(model.predict(x_train).T) # append predicted y-values for x_train to list\n",
    "    model.fit(x_train, y_train) # fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84adcb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print parameters and costs after all epochs\n",
    "print(\"weight: \" + str( model.params()[0]) )\n",
    "print(\"y-intercept: \" + str( model.params()[1]) )\n",
    "print(\"costs: \"+ str(model.cost(x_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which epochs/data points to plot\n",
    "a = np.arange(0, 50, 1).tolist()\n",
    "b = np.arange(50, 100, 5).tolist()\n",
    "c = np.arange(100, 12000, 200).tolist()\n",
    "p = a + b + c  # points we want to plot\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ecb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn lists into arrays\n",
    "w = np.array(w_list).flatten()\n",
    "b = np.array(b_list).flatten()\n",
    "c = np.array(c_list).flatten()\n",
    "ys = np.array(ys_list)\n",
    "p = np.array(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f1bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first animation:\n",
    "fig ,axs = plt.subplots(2,2,figsize=(10, 10))  # create figure\n",
    "#labelsize_ = 14\n",
    "camera = Camera(fig)  # create camera\n",
    "print(len(p))\n",
    "for idx , i in enumerate(p):\n",
    "    if idx%7==0:\n",
    "        print(\"ì§„í–‰ì¤‘:\",idx)\n",
    "        # ì„œë¸Œí”Œë¡¯ ì—…ë°ì´íŠ¸\n",
    "        axs[0, 0].plot(xs.T.flatten(), ys[i].flatten(), color='r')\n",
    "        axs[0, 0].scatter(x_train, y_train, color='b', marker='x', s=44)\n",
    "        axs[0, 0].set_title(f\"ì„ í˜• í”¼íŒ… - Epoch: {i}\")\n",
    "\n",
    "        axs[0, 1].plot(w[:i + 1], color='blue', linestyle=\"dashed\", alpha=0.5)\n",
    "        axs[0, 1].set_title(\"w\")\n",
    "\n",
    "        axs[1, 0].plot(b[:i + 1], color='red', linestyle=\"dashed\", alpha=0.5)\n",
    "        axs[1, 0].set_title(\"b\")\n",
    "\n",
    "        axs[1, 1].plot(c[:i + 1], color='black', linestyle=\"dashed\")\n",
    "        axs[1, 1].set_title(\"ë¹„ìš©\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        camera.snap()\n",
    "\n",
    "animation = camera.animate(interval=5,\n",
    "                           repeat=False, repeat_delay=500)  # create animation\n",
    "animation.save('media/SimpleLinReg_1.gif', writer='imagemagick')  # save animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7426f",
   "metadata": {},
   "source": [
    "# ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ê°€ í•˜ë‚˜ì¼ ë•Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ = 3\n",
    "b_ = 1\n",
    "\n",
    "for i in range(50000):\n",
    "    y_hat = w_ * x_train + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat), y^ = Wx + b\n",
    "    n = len(x_train) # ë°ì´í„° ê¸¸ì´\n",
    "    \n",
    "    # ë¯¸ë¶„\n",
    "    D_m = -1/(2*n) * sum(x_train * (y_train - y_hat)) # cost(J) ë¥¼ Wë¡œ í¸ë¯¸ë¶„(ê¸°ìš¸ê¸°)\n",
    "    D_b = -1/(2*n) * sum(y_train - y_hat) # cost(J) ë¥¼ bë¡œ í¸ë¯¸ë¶„ (yì ˆí¸)\n",
    "\n",
    "    # íŒŒë¼ë¯¸í„° ê°±ì‹ \n",
    "    w_ = w_ - 0.01 * D_m # wê°±ì‹ \n",
    "    b_ = b_ - 0.01 * D_b # bê°±ì‹ \n",
    "    \n",
    "    c_ = (1/n) * sum( (y_train - y_hat)**2 ) # cost = MSE\n",
    "    if i%1000==0:\n",
    "        print(w_, b_, c_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ae1ea",
   "metadata": {},
   "source": [
    "$$ J = \\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2 \\rightarrow MSE $$\n",
    "$$ \\frac{\\partial J} {\\partial w} = -\\frac{1}{2n}\\sum_{i=1}^{n}(y-\\hat{y})x $$\n",
    "$$ \\frac{\\partial J} {\\partial b} = -\\frac{1}{2n}\\sum_{i=1}^{n}(y-\\hat{y}) $$\n",
    "$$ w \\leftarrow w - \\alpha \\frac{\\partial J} {\\partial w} $$\n",
    "$$ b \\leftarrow b - \\alpha \\frac{\\partial J} {\\partial b} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da05d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[1],[2],[4],[5],[6],[7]])\n",
    "print(x_train.shape)\n",
    "\n",
    "y_train = np.array([[4],[-12],[3],[-11],[-5],[-17]])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ = 3\n",
    "b_ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45976f",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J} {\\partial w} = -\\frac{1}{2n}\\sum_{i=1}^{n}(y-\\hat{y})x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09982c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_w(x, y, w_, b_):\n",
    "    y_hat = w_ * x + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat), y^ = Wx + b\n",
    "    n = len(x) # ë°ì´í„° ê¸¸ì´\n",
    "    result = 0\n",
    "    for x_i, y_i, y_hat_i in zip(x, y, y_hat):\n",
    "        result += (y_i- y_hat_i) * x_i\n",
    "    return -1/(2*n)*result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caac0b7",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J} {\\partial b} = -\\frac{1}{2n}\\sum_{i=1}^{n}(y-\\hat{y}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_b(x, y, w_, b_):\n",
    "    y_hat = w_ * x + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat), y^ = Wx + b\n",
    "    n = len(x) # ë°ì´í„° ê¸¸ì´\n",
    "    result = 0\n",
    "    for y_i, y_hat_i in zip(y, y_hat):\n",
    "        result += (y_i - y_hat_i)\n",
    "    return -1/(2*n)*result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2c30e",
   "metadata": {},
   "source": [
    "$$ J = \\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2 \\rightarrow MSE $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b830627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x,y,w_,b_):\n",
    "    y_hat = w_ * x + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat), y^ = Wx + b\n",
    "    n = len(x) # ë°ì´í„° ê¸¸ì´\n",
    "    result = 0\n",
    "    for y_i, y_hat_i in zip(y, y_hat):\n",
    "         result += (y_i - y_hat_i)**2\n",
    "    return (1/n)*result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef6118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_ = 3\n",
    "b_ = 1\n",
    "\n",
    "for i in range(50000):\n",
    "    # D_b = (-1/n) * sum(y_train - y_pred) # cost(J) ë¥¼ bë¡œ í¸ë¯¸ë¶„ (yì ˆí¸)\n",
    "    dJ_dw = grad_w(x_train, y_train, w_, b_)\n",
    "    dJ_db = grad_b(x_train, y_train, w_, b_)\n",
    "    \n",
    "    # íŒŒë¼ë¯¸í„° ê°±ì‹ \n",
    "    w_ = w_ - 0.01 * dJ_dw # wê°±ì‹ \n",
    "    b_ = b_ - 0.01 * dJ_db # bê°±ì‹ \n",
    "    c_ = cost(x_train, y_train, w_, b_)\n",
    "    \n",
    "    if i%1000==0:\n",
    "        print(w_, b_, c_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644b01b0",
   "metadata": {},
   "source": [
    "# ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ê°€ ì—¬ëŸ¬ ê°œì´ë©°  ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘–ğ‘›ğ‘”ğ‘‘ğ‘ğ‘¡ğ‘(6ìŒ)\n",
    "$feature$ê°€ í•˜ë‚˜ì¼ ë•ŒëŠ” ê·¸ë‚˜ë§ˆ ê³ ì°¨ì› ìƒê°ì„ í•˜ì§€ ì•Šì•„ë„ ë˜ëŠ”ë° $feature$ê°€ ì—¬ëŸ¬ ê°œì´ê³  $training data(6ìŒ)$ì´ë¯€ë¡œ í–‰ë ¬ì˜ ì‚¬ê³ ë¥¼ ê°€ì ¸ì•¼í•œë‹¤.\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>feature 1</th>\n",
    "        <th>feature 2</th>\n",
    "        <th> label </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>3</td>\n",
    "        <td>4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>2</td>\n",
    "        <td>-12</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>2</td>\n",
    "        <td>3</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>-1</td>\n",
    "        <td>-2</td>\n",
    "        <td>-11</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>3</td>\n",
    "        <td>-5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>4</td>\n",
    "        <td>-7</td>\n",
    "    </tr>\n",
    "<table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b15c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_1 = 3\n",
    "w_2 = 2\n",
    "b_ = -1\n",
    "# ì´ˆê¸°ê°’ weight(w1, w2, bias(b)ì˜ ì´ˆê¸°ê°’) learing=> hyper parameter\n",
    "alpha = 0.01 # í•™ìŠµë¥ , learing_rate, í¬ë©´ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ì§€ë§Œ global mimimunì— ë„ë‹¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆì–´ìš”.)\n",
    "\n",
    "for i in range(100000):\n",
    "    y_hat = w_1 * x_train.T[0] + w_2 * x_train.T[1] + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat)\n",
    "    if i % 10000 == 0:\n",
    "        print('y_hat', y_hat.shape, 'y_train:', y_train.shape) # shapeì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ(1ì°¨ì› ë²¡í„°, 2ì°¨ì› í–‰ë ¬)\n",
    "        n = len(x_train)\n",
    "        \n",
    "    # ë¯¸ë¶„\n",
    "    DJ_dm1 = -1/n * sum(x_train.T[0] * (y_train.ravel() - y_hat)) # y_train.ravel()=> 2ì°¨ì›ì„ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n",
    "    DJ_dm2 = -1/n * sum(x_train.T[1] * (y_train.ravel() - y_hat)) # cost(J) ë¥¼ Wë¡œ í¸ë¯¸ë¶„(ê¸°ìš¸ê¸°)\n",
    "    D_b = -1/n * sum(y_train.ravel() - y_hat) # cost(J) ë¥¼ bë¡œ í¸ë¯¸ë¶„ (yì ˆí¸)\n",
    "    \n",
    "    # íŒŒë¼ë¯¸í„° ê°±ì‹ \n",
    "    w_1 = w_1 - alpha * DJ_dm1 # wê°±ì‹ \n",
    "    w_2 = w_2 - alpha * DJ_dm2 # wê°±ì‹ \n",
    "    b_ = b_ - alpha * D_b # bê°±ì‹ \n",
    "    \n",
    "    # ë¹„ìš© í•¨ìˆ˜ ê³„ì‚° ìˆ˜ì •\n",
    "    c_ = (1/n) * sum( (y_train.ravel() - y_hat)**2 )\n",
    "    \n",
    "    if i%10000 == 0:\n",
    "        print('w1:',w_1,'w2:', w_2,'b:', b_,'cost:', c_, 'ì˜¤ì°¨ë²”ìœ„:',abs(c_-0)<0.001)\n",
    "        print(\"=========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d379be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ê²€ì¦\n",
    "[(i[0] * 2.813459268004652 + i[1] * -1.455135773317597 + -9.066115702479113) for i, j in zip(x_train, y_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 1.5000000000000457\n",
    "w2 = 0.4999999999999684\n",
    "b = 1.9999999999999396\n",
    "for i, j in zip(x_train, y_train):\n",
    "    print(i[0]*w1 + i[1]*w2 + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697eb0c",
   "metadata": {},
   "source": [
    "$$ J = \\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2 \\rightarrow MSE $$\n",
    "$$ \\frac{\\partial J} {\\partial w} = -\\frac{1}{2n}\\sum_{i=1}^{n}(y-\\hat{y})x $$\n",
    "$$ \\frac{\\partial J} {\\partial b} = -\\frac{1}{2n}\\sum_{i=1}^{n}(y-\\hat{y}) $$\n",
    "$$ w \\leftarrow w - \\alpha \\frac{\\partial J} {\\partial w} $$\n",
    "$$ b \\leftarrow b - \\alpha \\frac{\\partial J} {\\partial b} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[1,1],[2,2],[3,3],[4,4],[5,5],[6,6]])\n",
    "print(x_train.shape) # w1=1, w2=1, b=2\n",
    "\n",
    "y_train = np.array([[4],[6],[8],[10],[12],[14]])\n",
    "print(y_train.shape) # label(ì •ë‹µ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc812b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = 3\n",
    "w_2 = 100\n",
    "b_ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e00b5a",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J} {\\partial w_{1}} = -\\frac{1}{2n}\\sum_{i=1}^{n}(y-\\hat{y})x_{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e2bad2",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J} {\\partial w_{2}} = -\\frac{1}{2n}\\sum_{i=1}^{n}(y-\\hat{y})x_{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af58eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_w1(x, y, w1_,w2_, b_):\n",
    "    y_hat = w1_ * x[0] + w2_ * x[1] + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat), y^ = Wx + b\n",
    "    n = len(x) # ë°ì´í„° ê¸¸ì´\n",
    "    result = 0\n",
    "    for x_i, y_i, y_hat_i in zip(x, y, y_hat):\n",
    "        result += (y_i- y_hat_i) * x_i\n",
    "    return -1/(2*n)*result\n",
    "\n",
    "def grad_w2(x, y, w1_,w2_, b_):\n",
    "    y_hat = w1_ * x[0] + w2_ * x[1] + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat), y^ = Wx + b\n",
    "    n = len(x) # ë°ì´í„° ê¸¸ì´\n",
    "    result = 0\n",
    "    for x_i, y_i, y_hat_i in zip(x, y, y_hat):\n",
    "        result += (y_i- y_hat_i) * x_i\n",
    "    return -1/(2*n)*result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84203ff",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J} {\\partial b} = -\\frac{1}{2n}\\sum_{i=1}^{n}(y-\\hat{y}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f619a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_b(x, y, w1_,w2_, b_):\n",
    "    y_hat = w1_ * x[0] + w2_ * x[1] + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat), y^ = Wx + b\n",
    "    n = len(x) # ë°ì´í„° ê¸¸ì´\n",
    "    result = 0\n",
    "    for y_i, y_hat_i in zip(y, y_hat):\n",
    "        result += (y_i - y_hat_i)\n",
    "    return -1/(2*n)*result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b9a6c",
   "metadata": {},
   "source": [
    "$$ \\hat{y} = w_{1}x_{1} + w_{2}x_{2} + b $$\n",
    "$$ J = \\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{y})^2 \\rightarrow MSE $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x,y,w1_,w2_,b_):\n",
    "    y_hat = w1_ * x[0] + w2_ * x[1] + b_ # ì˜ˆì¸¡ê°’(hypothesis, y_hat), y^ = Wx + b\n",
    "    n = len(x) # ë°ì´í„° ê¸¸ì´\n",
    "    result = 0\n",
    "    for y_i, y_hat_i in zip(y, y_hat):\n",
    "         result += (y_i - y_hat_i)**2\n",
    "    return (1/n)*result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041984d",
   "metadata": {},
   "source": [
    "$$ w_{1} \\leftarrow w_{1} - \\alpha \\frac{\\partial J} {\\partial w_{1}} $$\n",
    "$$ w_{2} \\leftarrow w_{2} - \\alpha \\frac{\\partial J} {\\partial w_{2}} $$\n",
    "$$ b \\leftarrow b - \\alpha \\frac{\\partial J} {\\partial b} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97464b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_ = 3\n",
    "w2_ = 100\n",
    "b_ = 1\n",
    "\n",
    "for i in range(50000):\n",
    "    # D_b = (-1/n) * sum(y_train - y_pred) # cost(J) ë¥¼ bë¡œ í¸ë¯¸ë¶„ (yì ˆí¸)\n",
    "    dJ_dw1 = grad_w1(x_train, y_train, w1_,w2_, b_)\n",
    "    dJ_dw2 = grad_w1(x_train, y_train, w1_,w2_, b_)\n",
    "    dJ_db = grad_b(x_train, y_train, w1_,w2_, b_)\n",
    "    \n",
    "    # íŒŒë¼ë¯¸í„° ê°±ì‹ \n",
    "    w1_ = w1_ - 0.01 * dJ_dw1 # wê°±ì‹ \n",
    "    w2_ = w2_ - 0.01 * dJ_dw1 # wê°±ì‹ \n",
    "    b_ = b_ - 0.01 * dJ_db # bê°±ì‹ \n",
    "    c_ = cost(x_train, y_train, w1_,w2_, b_)\n",
    "    \n",
    "    if i%1000==0:\n",
    "        print(w1_,w2_, b_, c_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a04c7db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e60848a",
   "metadata": {},
   "source": [
    "# logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons # ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import matplotlib.pyplot as plt # ê·¸ë¦¼ê·¸ë¦¬ê¸°\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f42f910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid (z): # (ğ‘¤â‹…ğ‘¥+ğ‘)\n",
    "    return 1.0/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f18467",
   "metadata": {},
   "source": [
    "## Hypothesis (ê°€ì„¤)\n",
    "$$ \\frac{1}{1+ e+{-(w \\cdot x+b})} $$\n",
    "\n",
    "## logistic regressionì˜ cost í•¨ìˆ˜ = \n",
    "$$ J(W,b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\widehat{y}^{(i)}, y^{(i)}) =\n",
    "    -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(\\widehat{y}^{(i)}+(1-y^{(i)})log(1-\\widehat{y}^{(i)})]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb04182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat): # Cross Entropy\n",
    "    loss = -np.mean(y*(np.log(y_hat)) - (1-y)*np.log(1-y_hat))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(X, y, y_hat):\n",
    "    # X ---> ì…ë ¥\n",
    "    # y ---> ì •ë‹µ(target / label)\n",
    "    # y_hat ---> ê°€ì„¤(ëª¨ë¸ì˜ ì¶œë ¥, hypothesis/ì˜ˆì¸¡ì¹˜(prediction/ì¶”ì •))\n",
    "    # w ---> weigth(íŒŒë¼ë¯¸í„°, theta / ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” ê°’)\n",
    "    # b ---> bias (íŒŒë¼ë¯¸í„°, theta 0 )\n",
    "    \n",
    "    # m ---> í•™ìŠµ(trainging) ë°ì´í„°ì˜ ê°¯ìˆ˜\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Cost(Loss)ë¥¼ weightë¡œ ë¯¸ë¶„í•¨\n",
    "    dw = (1/m)*np.dot(X.T, (y_hat - y))\n",
    "    \n",
    "    # Cost(Loss)ë¥¼ biasë¡œ ë¯¸ë¶„í•¨ \n",
    "    db = (1/m)*np.sum((y_hat - y))\n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addf2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, w, b):\n",
    "    # X = ì…ë ¥\n",
    "    # w = ê°€ì¤‘ì¹˜\n",
    "    # b = bais\n",
    "    \n",
    "    # ì§ì„ ì€  = mx + c\n",
    "    # ê·¸ë˜ì„œ ì§ì„ ì˜ ë°©ì •ì‹ mx + c = w.X + b    [.(ë‹·)ì€ ê³±í•œë‹¤ëŠ” ëœ») ]\n",
    "    # mê³¼ cë¥¼ í’€ì–´ë¼\n",
    "    x1 = [min(X[:, 0]), max(X[:, 0])] # X[:0] Xì˜ ëª¨ë“  í–‰ì„ ê°€ì ¸ì˜¤ê³  0ì—´ ì„ íƒ\n",
    "    m = -w[0]/w[1]\n",
    "    c = -b/w[1]\n",
    "    x2 = m*x1 + c\n",
    "    \n",
    "    # ê·¸ë¦¼ ê·¸ë¦¬ê¸°\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"g^\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n",
    "    plt.xlim([-2, 2]) # x êµ¬ê°„ ì§€ì •\n",
    "    plt.ylim([0, 2.2]) # ylimit (y êµ¬ê°„ì§€ì •)\n",
    "    plt.xlabel(\"feature 1\")\n",
    "    plt.ylabel(\"feature 2\")\n",
    "    plt.title('Decision Boundary')\n",
    "    plt.plot(x1, x2, 'y-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    # X <= ì…ë ¥\n",
    "    # m <= training ê°œìˆ˜\n",
    "    # n <= feature ì˜ ê°¯ìˆ˜ (weightì™€ ë‚´ì í•˜ëŠ” ê²ƒ)\n",
    "    m, n = X.shape # (m,n) mí–‰ nì—´\n",
    "    # Xí–‰ë ¬ì˜ ëª¨ë“  nê°œ featureë“¤ì„ ì •ê·œí™”í•¨\n",
    "    for i in range(n):\n",
    "        X = (X - X.mean(axis=0))/X.std(axis=0) \n",
    "        # ì–´ì œ í–ˆë˜ ìˆ˜ì‹(ë°ì´í„°ì™€ í‰ê· ì˜ ì°¨ì´ë¥¼) í‘œì¤€í¸ì°¨(stanard deviation)ìœ¼ë¡œ ë‚˜ëˆ”\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd03cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train í•¨ìˆ˜\n",
    "## ì´ í•¨ìˆ˜ì—ì„œ gradient descentë¥¼ ë°˜ë³µí•˜ì—¬ í•™ìŠµì„ í•˜ê³  weightì™€ biasë¥¼ êµ¬í•¨\n",
    "def train(X, y, bs, epochs, lr): \n",
    "    # X <- ì…ë ¥\n",
    "    # y <- true / target\n",
    "    # epochëŠ” ë°˜ë³µíšŸìˆ˜\n",
    "    # lr = learning rate(í•™ìŠµìœ¨)\n",
    "    \n",
    "    # m <- í•™ìŠµë°ì´í„°ì˜ ìˆ˜\n",
    "    # n <- featureì˜ ìˆ˜\n",
    "    \n",
    "    m, n = X.shape\n",
    "    # wieghtì™€ bias ì´ˆê¸°í™”\n",
    "    w = np.zeros((n, 1))\n",
    "    b = 0\n",
    "    # yë¥¼ reshapeí•¨(í˜•íƒœë¥¼ ë§ì¶¤)\n",
    "    y = y.reshape(m, 1)\n",
    "    # ì…ë ¥ ë°ì´í„° normalize\n",
    "    x = normalize(X)\n",
    "    # LOSSë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë¹ˆ LIST ìƒì„±\n",
    "    losses = []\n",
    "    # í•™ìŠµ\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((m-1)//bs + 1):\n",
    "            # batch ì •ì˜, SGD(Stocastric(í†µê³„ì 0), Gradient Descent)\n",
    "            start_i = i*bs # bs(ë°°ì¹˜ ì‚¬ì´ì¦ˆ)\n",
    "            end_i = start_i + bs\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            # hypothesis / ì˜ˆì¸¡ ê³„ì‚°\n",
    "            y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "            # lossë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë¯¸ë¶„\n",
    "            dw, db = gradients(xb, yb, y_hat)\n",
    "            # íŒŒë¼ë¯¸í„° ê°±ì‹ \n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "        l = loss(y, sigmoid(np.dot(X, w) + b))\n",
    "        losses.append(l)\n",
    "    # weightì™€ bias, loss ë°˜í™˜\n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fcb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def predict(X):\n",
    "    # X <- ì…ë ¥\n",
    "    # ì…ë ¥ ë°ì´í„° normalize\n",
    "    x = normalize(X)\n",
    "    \n",
    "    # ì˜ˆì¸¡ / ì¶”ì •ì¹˜ / y_hat ê³„ì‚°\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    # ì˜ˆì¸¡ ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "    pred_class = []\n",
    "    # y_hat >= 0.5 -> 1ë¡œ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "    # y_hat < 0.5 -> 0ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "    pred_class = [1 if i > 0.5 else 0 for i in preds]\n",
    "    return np.array(pre_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf618e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµ\n",
    "X, y = make_moons(n_samples=100, noise=0.24)\n",
    "w, b, l = train(X, y, bs=100, epochs=1000, lr=0.01)\n",
    "# ê·¸ë¦¼ ê·¸ë¦¬ê¸°\n",
    "plot_decision_boundary(X,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae94179",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.array(X[:10][:,0]) # y_hatì´ë¼ ê°€ì •í•©ë‹ˆë‹¤.\n",
    "print(np.shape(X_))\n",
    "\n",
    "y_ = np.array(y[:10])\n",
    "print(np.shape(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb3cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì œ) \n",
    "y_hat = np.array(X[:10][:,0]) # y_hatì´ë¼ ê°€ì •í•©ë‹ˆë‹¤.\n",
    "np.shape(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751671ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = [np.random.randint(10) for i in range(10)]\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e621a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true[0]* np.log(np.abs(y_hat[0])) + (1-y_true[0])*np.log(np.abs(1-y_hat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7cccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = 0\n",
    "for i, j in zip(y_true, y_hat):\n",
    "    cross_entropy += i* np.log(np.abs(j)) + (1-i)*np.log(np.abs(1-j)) \n",
    "count = len(y_true)\n",
    "-cross_entropy / count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf19a6",
   "metadata": {},
   "source": [
    "## ì†ì‹¤(Loss) / Cost(ë¹„ìš©) í•¨ìˆ˜\n",
    "## logistic regressionì˜ cost í•¨ìˆ˜ = \n",
    "$$ J(W,b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\widehat{y}^{(i)}, y^{(i)}) =\n",
    "    -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(\\widehat{y}^{(i)}+(1-y^{(i)})log(1-\\widehat{y}^{(i)})]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d7207",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=300, noise=0.24)\n",
    "print('X:', np.shape(X)) # Xë°ì´í„° (100í–‰ 2ì—´ í–‰ë ¬)\n",
    "print('y:', np.shape(X)) # Xë°ì´í„° (100í–‰ ë²¡í„°)\n",
    "bs = 15\n",
    "m = 20\n",
    "for epoch in range(10):\n",
    "    for i in range((m-1)//bs+1):\n",
    "        # batch ì •ì˜, SGD(Stocastric(í†µê³„ì 0) Gradient Descent)\n",
    "        start_i = i*bs # bs(ë°°ì¹˜ ì‚¬ì´ì¦ˆ)\n",
    "        end_i = start_i + bs\n",
    "        xb = X[start_i:end_i]\n",
    "        yb = y[start_i:end_i]\n",
    "        print('start: ', start_i, ', end: ', end_i, ', epoch: ', epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(-3,3,0.01)\n",
    "y = sigmoid(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c69f3",
   "metadata": {},
   "source": [
    "#### ë¯¸ë¶„ì˜ ì •ì˜ remind\n",
    "$$ \\lim_{h\\rightarrow 0} \\frac{f(x+h)-f(x)}{h} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3595fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(f, x):\n",
    "    h = 0.00001\n",
    "    return (f(x+h)-f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997031ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "derivated = derivative(sigmoid, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d7f10a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2f56b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X, derivated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567909e2",
   "metadata": {},
   "source": [
    "##  Hessian í–‰ë ¬\n",
    "$$   \\begin{bmatrix}\n",
    " \\frac{\\partial^{2}f}{\\partial x_{1}^2} &  \\frac{\\partial^{2}f}{\\partial x_{1} \\partial x_{2} }  & \\cdot  \\cdot \\cdot  &  \\frac{\\partial^{2}f}{\\partial x_{1} \\partial x_{n} }    \\\\\n",
    "\\frac{\\partial^{2}f}{\\partial x_{2} \\partial x_{1}} &\n",
    "\\frac{\\partial^{2}f}{\\partial x_{1}^2}  & \\cdot  \\cdot \\cdot  &  \n",
    "  \\frac{\\partial^{2}f}{\\partial x_{2}\\partial x_{n}}   \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial^{2}f}{\\partial x_{n} \\partial x_{1} } &  \\frac{\\partial^{2}f}{\\partial x_{n} \\partial x_{2} }  & \\cdot  \\cdot \\cdot  &  \\frac{\\partial^{2}f}{\\partial x_{n}^{2}}   \n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88246855",
   "metadata": {},
   "source": [
    "<a href=\"https://angeloyeo.github.io/2020/06/17/Hessian.html\">í—¤ì‹œì•ˆ í–‰ë ¬ì˜ ê¸°í•˜í•™ ì˜ë¯¸</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a566c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, Matrix, Function, simplify, exp, hessian, solve, init_printing\n",
    "x1, x2 = symbols('x1 x2')\n",
    "f, g, h = symbols('f g h', cls=Function)\n",
    "\n",
    "X = Matrix([x1, x2])\n",
    "f = Matrix([-x1*x2*exp(-(x1**2 + x2**2)/2)])\n",
    "h = 2*x1 + 3* x2\n",
    "g = x1**2 + x2**2-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradf = simplify(f.jacobian(X))\n",
    "gradf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian = simplify(hessian(f, X)) # hessian(ëŒ€ì¹­)\n",
    "hessian"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
